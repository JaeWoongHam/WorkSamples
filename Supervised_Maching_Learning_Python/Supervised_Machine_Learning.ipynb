{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Introduction\n",
    "\n",
    "This is a demonstration of supervised machine learning methods for a continous target variable. The data used is the California Test Score Data Set extracted from the link below. It is a subset of data from the California Department of Education from 1998 to 1999. The target variable is \"testscr\", which is the average test score for a student. This section explores what variables have the highest predictive power in determining the student's test score using the various supervised learning models available.\n",
    "\n",
    "Data Source Link: \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Caschool.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>distcod</th>\n",
       "      <th>county</th>\n",
       "      <th>district</th>\n",
       "      <th>grspan</th>\n",
       "      <th>enrltot</th>\n",
       "      <th>teachers</th>\n",
       "      <th>calwpct</th>\n",
       "      <th>mealpct</th>\n",
       "      <th>computer</th>\n",
       "      <th>testscr</th>\n",
       "      <th>compstu</th>\n",
       "      <th>expnstu</th>\n",
       "      <th>str</th>\n",
       "      <th>avginc</th>\n",
       "      <th>elpct</th>\n",
       "      <th>readscr</th>\n",
       "      <th>mathscr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>75119</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>Sunol Glen Unified</td>\n",
       "      <td>KK-08</td>\n",
       "      <td>195</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>2.040800</td>\n",
       "      <td>67</td>\n",
       "      <td>690.799988</td>\n",
       "      <td>0.343590</td>\n",
       "      <td>6384.911133</td>\n",
       "      <td>17.889910</td>\n",
       "      <td>22.690001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>691.599976</td>\n",
       "      <td>690.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>61499</td>\n",
       "      <td>Butte</td>\n",
       "      <td>Manzanita Elementary</td>\n",
       "      <td>KK-08</td>\n",
       "      <td>240</td>\n",
       "      <td>11.150000</td>\n",
       "      <td>15.416700</td>\n",
       "      <td>47.916698</td>\n",
       "      <td>101</td>\n",
       "      <td>661.200012</td>\n",
       "      <td>0.420833</td>\n",
       "      <td>5099.380859</td>\n",
       "      <td>21.524664</td>\n",
       "      <td>9.824000</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>660.500000</td>\n",
       "      <td>661.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>61549</td>\n",
       "      <td>Butte</td>\n",
       "      <td>Thermalito Union Elementary</td>\n",
       "      <td>KK-08</td>\n",
       "      <td>1550</td>\n",
       "      <td>82.900002</td>\n",
       "      <td>55.032299</td>\n",
       "      <td>76.322601</td>\n",
       "      <td>169</td>\n",
       "      <td>643.599976</td>\n",
       "      <td>0.109032</td>\n",
       "      <td>5501.954590</td>\n",
       "      <td>18.697226</td>\n",
       "      <td>8.978000</td>\n",
       "      <td>30.000002</td>\n",
       "      <td>636.299988</td>\n",
       "      <td>650.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>61457</td>\n",
       "      <td>Butte</td>\n",
       "      <td>Golden Feather Union Elementary</td>\n",
       "      <td>KK-08</td>\n",
       "      <td>243</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>36.475399</td>\n",
       "      <td>77.049202</td>\n",
       "      <td>85</td>\n",
       "      <td>647.700012</td>\n",
       "      <td>0.349794</td>\n",
       "      <td>7101.831055</td>\n",
       "      <td>17.357143</td>\n",
       "      <td>8.978000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>651.900024</td>\n",
       "      <td>643.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>61523</td>\n",
       "      <td>Butte</td>\n",
       "      <td>Palermo Union Elementary</td>\n",
       "      <td>KK-08</td>\n",
       "      <td>1335</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>33.108601</td>\n",
       "      <td>78.427002</td>\n",
       "      <td>171</td>\n",
       "      <td>640.849976</td>\n",
       "      <td>0.128090</td>\n",
       "      <td>5235.987793</td>\n",
       "      <td>18.671329</td>\n",
       "      <td>9.080333</td>\n",
       "      <td>13.857677</td>\n",
       "      <td>641.799988</td>\n",
       "      <td>639.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>62042</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>Burrel Union Elementary</td>\n",
       "      <td>KK-08</td>\n",
       "      <td>137</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>12.318800</td>\n",
       "      <td>86.956497</td>\n",
       "      <td>25</td>\n",
       "      <td>605.550049</td>\n",
       "      <td>0.182482</td>\n",
       "      <td>5580.146973</td>\n",
       "      <td>21.406250</td>\n",
       "      <td>10.415000</td>\n",
       "      <td>12.408759</td>\n",
       "      <td>605.700012</td>\n",
       "      <td>605.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>68536</td>\n",
       "      <td>San Joaquin</td>\n",
       "      <td>Holt Union Elementary</td>\n",
       "      <td>KK-08</td>\n",
       "      <td>195</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.903200</td>\n",
       "      <td>94.623703</td>\n",
       "      <td>28</td>\n",
       "      <td>606.750000</td>\n",
       "      <td>0.143590</td>\n",
       "      <td>5253.331055</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>6.577000</td>\n",
       "      <td>68.717949</td>\n",
       "      <td>604.500000</td>\n",
       "      <td>609.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>63834</td>\n",
       "      <td>Kern</td>\n",
       "      <td>Vineland Elementary</td>\n",
       "      <td>KK-08</td>\n",
       "      <td>888</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>18.806299</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>0.074324</td>\n",
       "      <td>4565.746094</td>\n",
       "      <td>20.894117</td>\n",
       "      <td>8.174000</td>\n",
       "      <td>46.959461</td>\n",
       "      <td>605.500000</td>\n",
       "      <td>612.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>62331</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>Orange Center Elementary</td>\n",
       "      <td>KK-08</td>\n",
       "      <td>379</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>32.189999</td>\n",
       "      <td>93.139801</td>\n",
       "      <td>35</td>\n",
       "      <td>612.500000</td>\n",
       "      <td>0.092348</td>\n",
       "      <td>5355.548340</td>\n",
       "      <td>19.947369</td>\n",
       "      <td>7.385000</td>\n",
       "      <td>30.079157</td>\n",
       "      <td>608.900024</td>\n",
       "      <td>616.099976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>67306</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>Del Paso Heights Elementary</td>\n",
       "      <td>KK-06</td>\n",
       "      <td>2247</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>78.994202</td>\n",
       "      <td>87.316399</td>\n",
       "      <td>0</td>\n",
       "      <td>612.650024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5036.211426</td>\n",
       "      <td>20.805555</td>\n",
       "      <td>11.613333</td>\n",
       "      <td>40.275921</td>\n",
       "      <td>611.900024</td>\n",
       "      <td>613.400024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  distcod       county                         district grspan  \\\n",
       "0           1    75119      Alameda               Sunol Glen Unified  KK-08   \n",
       "1           2    61499        Butte             Manzanita Elementary  KK-08   \n",
       "2           3    61549        Butte      Thermalito Union Elementary  KK-08   \n",
       "3           4    61457        Butte  Golden Feather Union Elementary  KK-08   \n",
       "4           5    61523        Butte         Palermo Union Elementary  KK-08   \n",
       "5           6    62042       Fresno          Burrel Union Elementary  KK-08   \n",
       "6           7    68536  San Joaquin            Holt Union Elementary  KK-08   \n",
       "7           8    63834         Kern              Vineland Elementary  KK-08   \n",
       "8           9    62331       Fresno         Orange Center Elementary  KK-08   \n",
       "9          10    67306   Sacramento      Del Paso Heights Elementary  KK-06   \n",
       "\n",
       "   enrltot    teachers    calwpct     mealpct  computer     testscr   compstu  \\\n",
       "0      195   10.900000   0.510200    2.040800        67  690.799988  0.343590   \n",
       "1      240   11.150000  15.416700   47.916698       101  661.200012  0.420833   \n",
       "2     1550   82.900002  55.032299   76.322601       169  643.599976  0.109032   \n",
       "3      243   14.000000  36.475399   77.049202        85  647.700012  0.349794   \n",
       "4     1335   71.500000  33.108601   78.427002       171  640.849976  0.128090   \n",
       "5      137    6.400000  12.318800   86.956497        25  605.550049  0.182482   \n",
       "6      195   10.000000  12.903200   94.623703        28  606.750000  0.143590   \n",
       "7      888   42.500000  18.806299  100.000000        66  609.000000  0.074324   \n",
       "8      379   19.000000  32.189999   93.139801        35  612.500000  0.092348   \n",
       "9     2247  108.000000  78.994202   87.316399         0  612.650024  0.000000   \n",
       "\n",
       "       expnstu        str     avginc      elpct     readscr     mathscr  \n",
       "0  6384.911133  17.889910  22.690001   0.000000  691.599976  690.000000  \n",
       "1  5099.380859  21.524664   9.824000   4.583333  660.500000  661.900024  \n",
       "2  5501.954590  18.697226   8.978000  30.000002  636.299988  650.900024  \n",
       "3  7101.831055  17.357143   8.978000   0.000000  651.900024  643.500000  \n",
       "4  5235.987793  18.671329   9.080333  13.857677  641.799988  639.900024  \n",
       "5  5580.146973  21.406250  10.415000  12.408759  605.700012  605.400024  \n",
       "6  5253.331055  19.500000   6.577000  68.717949  604.500000  609.000000  \n",
       "7  4565.746094  20.894117   8.174000  46.959461  605.500000  612.500000  \n",
       "8  5355.548340  19.947369   7.385000  30.079157  608.900024  616.099976  \n",
       "9  5036.211426  20.805555  11.613333  40.275921  611.900024  613.400024  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the data\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Caschool.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enrltot</th>\n",
       "      <th>teachers</th>\n",
       "      <th>calwpct</th>\n",
       "      <th>mealpct</th>\n",
       "      <th>computer</th>\n",
       "      <th>compstu</th>\n",
       "      <th>expnstu</th>\n",
       "      <th>str</th>\n",
       "      <th>avginc</th>\n",
       "      <th>elpct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>2.040800</td>\n",
       "      <td>67</td>\n",
       "      <td>0.343590</td>\n",
       "      <td>6384.911133</td>\n",
       "      <td>17.889910</td>\n",
       "      <td>22.690001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>240</td>\n",
       "      <td>11.150000</td>\n",
       "      <td>15.416700</td>\n",
       "      <td>47.916698</td>\n",
       "      <td>101</td>\n",
       "      <td>0.420833</td>\n",
       "      <td>5099.380859</td>\n",
       "      <td>21.524664</td>\n",
       "      <td>9.824000</td>\n",
       "      <td>4.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1550</td>\n",
       "      <td>82.900002</td>\n",
       "      <td>55.032299</td>\n",
       "      <td>76.322601</td>\n",
       "      <td>169</td>\n",
       "      <td>0.109032</td>\n",
       "      <td>5501.954590</td>\n",
       "      <td>18.697226</td>\n",
       "      <td>8.978000</td>\n",
       "      <td>30.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>36.475399</td>\n",
       "      <td>77.049202</td>\n",
       "      <td>85</td>\n",
       "      <td>0.349794</td>\n",
       "      <td>7101.831055</td>\n",
       "      <td>17.357143</td>\n",
       "      <td>8.978000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1335</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>33.108601</td>\n",
       "      <td>78.427002</td>\n",
       "      <td>171</td>\n",
       "      <td>0.128090</td>\n",
       "      <td>5235.987793</td>\n",
       "      <td>18.671329</td>\n",
       "      <td>9.080333</td>\n",
       "      <td>13.857677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>137</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>12.318800</td>\n",
       "      <td>86.956497</td>\n",
       "      <td>25</td>\n",
       "      <td>0.182482</td>\n",
       "      <td>5580.146973</td>\n",
       "      <td>21.406250</td>\n",
       "      <td>10.415000</td>\n",
       "      <td>12.408759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>195</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.903200</td>\n",
       "      <td>94.623703</td>\n",
       "      <td>28</td>\n",
       "      <td>0.143590</td>\n",
       "      <td>5253.331055</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>6.577000</td>\n",
       "      <td>68.717949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>888</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>18.806299</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66</td>\n",
       "      <td>0.074324</td>\n",
       "      <td>4565.746094</td>\n",
       "      <td>20.894117</td>\n",
       "      <td>8.174000</td>\n",
       "      <td>46.959461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>379</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>32.189999</td>\n",
       "      <td>93.139801</td>\n",
       "      <td>35</td>\n",
       "      <td>0.092348</td>\n",
       "      <td>5355.548340</td>\n",
       "      <td>19.947369</td>\n",
       "      <td>7.385000</td>\n",
       "      <td>30.079157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2247</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>78.994202</td>\n",
       "      <td>87.316399</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5036.211426</td>\n",
       "      <td>20.805555</td>\n",
       "      <td>11.613333</td>\n",
       "      <td>40.275921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   enrltot    teachers    calwpct     mealpct  computer   compstu  \\\n",
       "0      195   10.900000   0.510200    2.040800        67  0.343590   \n",
       "1      240   11.150000  15.416700   47.916698       101  0.420833   \n",
       "2     1550   82.900002  55.032299   76.322601       169  0.109032   \n",
       "3      243   14.000000  36.475399   77.049202        85  0.349794   \n",
       "4     1335   71.500000  33.108601   78.427002       171  0.128090   \n",
       "5      137    6.400000  12.318800   86.956497        25  0.182482   \n",
       "6      195   10.000000  12.903200   94.623703        28  0.143590   \n",
       "7      888   42.500000  18.806299  100.000000        66  0.074324   \n",
       "8      379   19.000000  32.189999   93.139801        35  0.092348   \n",
       "9     2247  108.000000  78.994202   87.316399         0  0.000000   \n",
       "\n",
       "       expnstu        str     avginc      elpct  \n",
       "0  6384.911133  17.889910  22.690001   0.000000  \n",
       "1  5099.380859  21.524664   9.824000   4.583333  \n",
       "2  5501.954590  18.697226   8.978000  30.000002  \n",
       "3  7101.831055  17.357143   8.978000   0.000000  \n",
       "4  5235.987793  18.671329   9.080333  13.857677  \n",
       "5  5580.146973  21.406250  10.415000  12.408759  \n",
       "6  5253.331055  19.500000   6.577000  68.717949  \n",
       "7  4565.746094  20.894117   8.174000  46.959461  \n",
       "8  5355.548340  19.947369   7.385000  30.079157  \n",
       "9  5036.211426  20.805555  11.613333  40.275921  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the data\n",
    "\n",
    "df = df.iloc[:, 5:] # deletes the first five columns, as the first coumn just denotes number assignment and the next four variables are categorical/discrete in nature\n",
    "y = df['testscr'] # since average test score is our target variable\n",
    "X = df.loc[:, (df.columns != 'testscr') & (df.columns != 'readscr') & (df.columns != 'mathscr')] #we are testing for all variables that are not testscr, readscr, mathscr\n",
    "\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) #splitting the data between test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.303\n",
      "Test set score: 0.085\n",
      "R squared: -0.074\n"
     ]
    }
   ],
   "source": [
    "# KNN Regressor Code Block\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(knn.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn.score(X_test, y_test)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(knn, X_train, y_train, cv = 10, scoring=\"r2\"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.805\n",
      "Test set score: 0.807\n",
      "lr.coef_: [ 2.37299343e-04 -9.68465212e-03 -1.05122232e-01 -3.63632211e-01\n",
      "  2.23981150e-03 -1.50110373e+00  1.87297506e-03 -3.68530746e-01\n",
      "  5.17180694e-01 -1.97462843e-01]\n",
      "lr.intercept_: 664.1994881014358\n",
      "R squared: 0.779\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Code Block\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lr.score(X_test, y_test)))\n",
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(lr, X_train, y_train, cv=10, scoring=\"r2\"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.805\n",
      "Test set score: 0.807\n",
      "Number of features used: 10\n",
      "ridge.coef_: [ 2.36884983e-04 -9.66951839e-03 -1.05109406e-01 -3.63639497e-01\n",
      "  2.23703012e-03 -1.48447285e+00  1.87274445e-03 -3.68401059e-01\n",
      "  5.17171949e-01 -1.97451767e-01]\n",
      "R squared: 0.779\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression Code Block\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=0.01).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(ridge.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(ridge.coef_ != 0)))\n",
    "print(\"ridge.coef_: {}\".format(ridge.coef_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(ridge, X_train, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.805\n",
      "Test set score: 0.808\n",
      "Number of features used: 9\n",
      "lasso.coef_: [ 1.84493637e-04 -7.99498408e-03 -1.03868379e-01 -3.64418034e-01\n",
      "  1.98214887e-03 -0.00000000e+00  1.86123551e-03 -3.50988030e-01\n",
      "  5.16083603e-01 -1.96387846e-01]\n",
      "R squared: 0.780\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression Code Block\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\n",
    "print(\"lasso.coef_: {}\".format(lasso.coef_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(lasso, X_train, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data using StandardScaler\n",
    "\n",
    "When comparing the set scores (test and training) and the overall r^2 between the non-scaled and scaled results, some showed improvements, while others didn't. This makes sense because standard scaler conceptually transforms that data, where it has a mean of 0 and standard deviation of 1. In other words, it rearranges the data to a normal distribution. As such, it proves to be more effective for KNN than regression fitting, as demonstrated from the results below. In fact, it is better to normalize than using a standard scaler when dealing with regression. This is primarily because it normalizes the data to decrease large range and values. In this way, it can reduce numerical instabilities.\n",
    "\n",
    "1) KNN Regressor\n",
    "<br>\n",
    "Trainining set score: 0.303 to 0.809\n",
    "<br>\n",
    "Test set score: 0.085 to 0.769\n",
    "<br>\n",
    "R sqaured: -0.074 to 0.709\n",
    "\n",
    "2) Linear Regression\n",
    "<br>\n",
    "Training set score: 0.805 to 0.805\n",
    "<br>\n",
    "Test set score: 0.807 to 0.807\n",
    "<br>\n",
    "R squared: 0.779 to 0.779\n",
    "\n",
    "3) Ridge Regression\n",
    "<br>\n",
    "Training set score: 0.805 to 0.805\n",
    "<br>\n",
    "Test set score: 0.807 to 0.807\n",
    "<br>\n",
    "R squared: 0.779 to 0.779\n",
    "\n",
    "4) Lasso Regression\n",
    "<br>\n",
    "Training set score: 0.805 to 0.805\n",
    "<br>\n",
    "Test set score: 0.808 to 0.807\n",
    "<br>\n",
    "R squared: 0.780 to 0.782"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enrltot</th>\n",
       "      <th>teachers</th>\n",
       "      <th>calwpct</th>\n",
       "      <th>mealpct</th>\n",
       "      <th>computer</th>\n",
       "      <th>compstu</th>\n",
       "      <th>expnstu</th>\n",
       "      <th>str</th>\n",
       "      <th>avginc</th>\n",
       "      <th>elpct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>36.475399</td>\n",
       "      <td>77.049202</td>\n",
       "      <td>85</td>\n",
       "      <td>0.349794</td>\n",
       "      <td>7101.831055</td>\n",
       "      <td>17.357143</td>\n",
       "      <td>8.978000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6880</td>\n",
       "      <td>303.029999</td>\n",
       "      <td>21.282400</td>\n",
       "      <td>94.971199</td>\n",
       "      <td>960</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>5064.615723</td>\n",
       "      <td>22.704023</td>\n",
       "      <td>7.022000</td>\n",
       "      <td>77.005814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>141</td>\n",
       "      <td>6.910000</td>\n",
       "      <td>10.071900</td>\n",
       "      <td>9.352500</td>\n",
       "      <td>44</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>6060.256836</td>\n",
       "      <td>20.405210</td>\n",
       "      <td>20.089001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>723</td>\n",
       "      <td>37.119999</td>\n",
       "      <td>25.921101</td>\n",
       "      <td>83.157898</td>\n",
       "      <td>45</td>\n",
       "      <td>0.062241</td>\n",
       "      <td>4692.493652</td>\n",
       "      <td>19.477371</td>\n",
       "      <td>8.279000</td>\n",
       "      <td>36.929462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>307</td>\n",
       "      <td>15.850000</td>\n",
       "      <td>5.537500</td>\n",
       "      <td>28.664499</td>\n",
       "      <td>36</td>\n",
       "      <td>0.117264</td>\n",
       "      <td>4718.163086</td>\n",
       "      <td>19.369085</td>\n",
       "      <td>14.578000</td>\n",
       "      <td>7.491857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2019</td>\n",
       "      <td>102.779999</td>\n",
       "      <td>10.252600</td>\n",
       "      <td>40.564602</td>\n",
       "      <td>167</td>\n",
       "      <td>0.082714</td>\n",
       "      <td>5193.692383</td>\n",
       "      <td>19.643900</td>\n",
       "      <td>11.238000</td>\n",
       "      <td>5.943536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3017</td>\n",
       "      <td>138.500000</td>\n",
       "      <td>14.976800</td>\n",
       "      <td>56.063599</td>\n",
       "      <td>496</td>\n",
       "      <td>0.164402</td>\n",
       "      <td>4675.674805</td>\n",
       "      <td>21.783394</td>\n",
       "      <td>11.081000</td>\n",
       "      <td>17.003647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>205</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>18.536600</td>\n",
       "      <td>80.975601</td>\n",
       "      <td>24</td>\n",
       "      <td>0.117073</td>\n",
       "      <td>4895.439453</td>\n",
       "      <td>18.303572</td>\n",
       "      <td>8.258000</td>\n",
       "      <td>41.463413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>8432</td>\n",
       "      <td>360.500000</td>\n",
       "      <td>6.392300</td>\n",
       "      <td>22.983900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5397.689453</td>\n",
       "      <td>23.389736</td>\n",
       "      <td>14.097667</td>\n",
       "      <td>4.969165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>244</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.573800</td>\n",
       "      <td>40.163898</td>\n",
       "      <td>15</td>\n",
       "      <td>0.061475</td>\n",
       "      <td>5118.373047</td>\n",
       "      <td>22.181818</td>\n",
       "      <td>12.640000</td>\n",
       "      <td>6.967213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     enrltot    teachers    calwpct    mealpct  computer   compstu  \\\n",
       "3        243   14.000000  36.475399  77.049202        85  0.349794   \n",
       "18      6880  303.029999  21.282400  94.971199       960  0.139535   \n",
       "393      141    6.910000  10.071900   9.352500        44  0.312057   \n",
       "60       723   37.119999  25.921101  83.157898        45  0.062241   \n",
       "203      307   15.850000   5.537500  28.664499        36  0.117264   \n",
       "154     2019  102.779999  10.252600  40.564602       167  0.082714   \n",
       "63      3017  138.500000  14.976800  56.063599       496  0.164402   \n",
       "110      205   11.200000  18.536600  80.975601        24  0.117073   \n",
       "311     8432  360.500000   6.392300  22.983900         0  0.000000   \n",
       "312      244   11.000000  15.573800  40.163898        15  0.061475   \n",
       "\n",
       "         expnstu        str     avginc      elpct  \n",
       "3    7101.831055  17.357143   8.978000   0.000000  \n",
       "18   5064.615723  22.704023   7.022000  77.005814  \n",
       "393  6060.256836  20.405210  20.089001   0.000000  \n",
       "60   4692.493652  19.477371   8.279000  36.929462  \n",
       "203  4718.163086  19.369085  14.578000   7.491857  \n",
       "154  5193.692383  19.643900  11.238000   5.943536  \n",
       "63   4675.674805  21.783394  11.081000  17.003647  \n",
       "110  4895.439453  18.303572   8.258000  41.463413  \n",
       "311  5397.689453  23.389736  14.097667   4.969165  \n",
       "312  5118.373047  22.181818  12.640000   6.967213  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaling the data using Standard Scalar\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.809\n",
      "Test set score: 0.769\n",
      "R squared: 0.709\n"
     ]
    }
   ],
   "source": [
    "# KNN Regressor Scaled Fit Code Block\n",
    "knn_scaled = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(knn_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(knn_scaled, X_train_scaled, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.805\n",
      "Test set score: 0.807\n",
      "lr.coef_: [ 0.99068803 -1.93639614 -1.17333052 -9.77646224  1.04183669 -0.09712146\n",
      "  1.15380605 -0.67884963  3.71960199 -3.4784803 ]\n",
      "lr.intercept_: 654.2720646933902\n",
      "R squared: 0.779\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Scaled Fit Code Block\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_scaled = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(lr_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lr_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"lr.coef_: {}\".format(lr_scaled.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr_scaled.intercept_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(lr_scaled, X_train_scaled, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.805\n",
      "Test set score: 0.807\n",
      "Number of features used: 10\n",
      "ridge.coef_: [ 0.96617956 -1.90993807 -1.17423865 -9.77520551  1.03962552 -0.09676499\n",
      "  1.15398941 -0.67778551  3.71966446 -3.47893586]\n",
      "R squared: 0.779\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression Scaled Fit Code Block\n",
    "ridge_scaled = Ridge(alpha=0.01).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(ridge_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(ridge_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(ridge_scaled.coef_ != 0)))\n",
    "print(\"ridge.coef_: {}\".format(ridge_scaled.coef_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(ridge_scaled, X_train_scaled, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.805\n",
      "Test set score: 0.807\n",
      "Number of features used: 9\n",
      "lasso.coef_: [-0.         -0.68154738 -1.16302274 -9.79903359  0.76196085 -0.03307188\n",
      "  1.14805701 -0.62654296  3.70821439 -3.46159671]\n",
      "R squared: 0.782\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression Scaled Fit Code Block\n",
    "lasso_scaled = Lasso(alpha=0.01, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(lasso_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lasso_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso_scaled.coef_ != 0)))\n",
    "print(\"lasso.coef_: {}\".format(lasso_scaled.coef_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(lasso_scaled, X_train_scaled, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-tuning parameters using GridSearchCV\n",
    "\n",
    "I have compared the results from the model when applying the GridSearch CV for both scaled and unscaled variables. Even though it proves to be more work, the comparison between both scaled and unscaled variables for different models will provide a more complete analysis. In terms of interpreting the brief results below, the output number is always compared to the initial unaltered original model results.\n",
    "\n",
    "In regards to the actual results, KNN classification had a drastic improvement in its fit for the scaled output, again. However, the tuning of its parameters did not have much of an impact on the regression models in general, as lasso and ridge regressions already regularized its parameters, and have pushed its coefficients toward zero due to the mathematical algorithim of how regression and lasso regressions are computed.\n",
    "<br>\n",
    "<br>\n",
    "1) KNN Regressor (Unnscaled, Optimized Parameters)\n",
    "<br>\n",
    "Trainining set score: 0.273 to 0.157\n",
    "<br>\n",
    "Test set score: 0.038 to 0.034\n",
    "<br>\n",
    "R squared: -0.271 to 0.022\n",
    "\n",
    "KNN Regressor (Scaled, Optimized Parameters)\n",
    "<br>\n",
    "Trainining set score: 0.273 to 0.773\n",
    "<br>\n",
    "Test set score: 0.038 to 0.760\n",
    "<br>\n",
    "R squared: -0.271 to 0.728\n",
    "\n",
    "\n",
    "2) Linear Regression (Unnscaled, Optimized Parameters)\n",
    "<br>\n",
    "Training set score: 0.805 to 0.805\n",
    "<br>\n",
    "Test set score: 0.807 to 0.807\n",
    "<br>\n",
    "R squared: 0.779 to 0.779\n",
    "\n",
    "Linear Regression (Scaled, Optimized Parameters)\n",
    "<br>\n",
    "Training set score: 0.805 to 0.805\n",
    "<br>\n",
    "Test set score: 0.807 to 0.807\n",
    "<br>\n",
    "R squared: 0.779 to 0.779\n",
    "\n",
    "3) Ridge Regression (Unnscaled, Optimized Parameters)\n",
    "<br>\n",
    "Training set score: 0.805 to 0.805\n",
    "<br>\n",
    "Test set score: 0.807 to 0.807\n",
    "<br>\n",
    "R squared: 0.779 to 0.780\n",
    "\n",
    "Ridge Regression (Scaled, Optimized Parameters)\n",
    "<br>\n",
    "Training set score: 0.805 to 0.805\n",
    "<br>\n",
    "Test set score: 0.807 to 0.807\n",
    "<br>\n",
    "R squared: 0.779 to 0.782\n",
    "\n",
    "4) Lasso Regression (Unnscaled, Optimized Parameters)\n",
    "<br>\n",
    "Training set score: 0.805 to 0.805\n",
    "<br>\n",
    "Test set score: 0.808 to 0.807\n",
    "<br>\n",
    "R squared: 0.780 to 0.783\n",
    "\n",
    "Lasso Regression (Scaled, Optimized Parameters)\n",
    "<br>\n",
    "Training set score: 0.805 to 0.797\n",
    "<br>\n",
    "Test set score: 0.808 to 0.786\n",
    "<br>\n",
    "R squared: 0.780 to 0.780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.021\n",
      "Best parameters: {'n_neighbors': 13}\n",
      "Test set score: 0.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_neighbors': np.arange(1, 15, 2)}\n",
    "grid = GridSearchCV(KNeighborsRegressor(), param_grid=param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Test set score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.157\n",
      "Test set score: 0.034\n",
      "R squared: 0.022\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=13)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(knn.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn.score(X_test, y_test)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(knn, X_train, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.728\n",
      "Best parameters: {'n_neighbors': 13}\n",
      "Test set score: 0.760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(1, 15, 2)}\n",
    "grid = GridSearchCV(KNeighborsRegressor(), param_grid=param_grid, cv=10)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Test set score: {:.3f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.773\n",
      "Test set score: 0.760\n",
      "R squared: 0.728\n"
     ]
    }
   ],
   "source": [
    "knn_scaled = KNeighborsRegressor(n_neighbors=13)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(knn_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(knn_scaled, X_train_scaled, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.779\n",
      "Best parameters: {'copy_X': True, 'fit_intercept': True, 'normalize': True}\n",
      "Test set score: 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n",
    "grid = GridSearchCV(LinearRegression(),parameters, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Test set score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.805\n",
      "Test set score: 0.807\n",
      "lr.coef_: [ 2.37299343e-04 -9.68465212e-03 -1.05122232e-01 -3.63632211e-01\n",
      "  2.23981150e-03 -1.50110373e+00  1.87297506e-03 -3.68530746e-01\n",
      "  5.17180694e-01 -1.97462843e-01]\n",
      "lr.intercept_: 664.1994881014356\n",
      "R squared: 0.779\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(fit_intercept = True, normalize = True, copy_X = True).fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lr.score(X_test, y_test)))\n",
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(lr, X_train, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.779\n",
      "Best parameters: {'copy_X': True, 'fit_intercept': True, 'normalize': False}\n",
      "Test set score: 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n",
    "grid = GridSearchCV(LinearRegression(),parameters, cv=10)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(\"Best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Test set score: {:.3f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.805\n",
      "Test set score: 0.807\n",
      "lr.coef_: [ 0.99068803 -1.93639614 -1.17333052 -9.77646224  1.04183669 -0.09712146\n",
      "  1.15380605 -0.67884963  3.71960199 -3.4784803 ]\n",
      "lr.intercept_: 654.2720646933902\n",
      "R squared: 0.779\n"
     ]
    }
   ],
   "source": [
    "lr_scaled = LinearRegression(fit_intercept = True, normalize = False, copy_X = True).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(lr_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lr_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"lr.coef_: {}\".format(lr_scaled.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr_scaled.intercept_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(lr_scaled, X_train_scaled, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.778\n",
      "Best parameters: 1.0\n",
      "Test set score: 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "alphas = np.array([1,0.1,0.01,0.001,0.0001,0]) #list of alpha values to test\n",
    "\n",
    "grid = GridSearchCV(estimator=Ridge(max_iter=100000), param_grid=dict(alpha=alphas))\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: {}\".format(grid.best_estimator_.alpha))\n",
    "print(\"Test set score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.805\n",
      "Test set score: 0.807\n",
      "Number of features used: 10\n",
      "ridge.coef_: [ 2.16651495e-04 -8.94419019e-03 -1.04514474e-01 -3.63985541e-01\n",
      "  2.10678551e-03 -7.07149871e-01  1.86261011e-03 -3.61969722e-01\n",
      "  5.16732038e-01 -1.96932369e-01]\n",
      "R squared: 0.780\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha=1.0, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(ridge.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(ridge.coef_ != 0)))\n",
    "print(\"ridge.coef_: {}\".format(ridge.coef_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(ridge, X_train, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.779\n",
      "Best parameters: 1.0\n",
      "Test set score: 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=Ridge(max_iter=100000), param_grid=dict(alpha=alphas))\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(\"Best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: {}\".format(grid.best_estimator_.alpha))\n",
    "print(\"Test set score: {:.3f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.805\n",
      "Test set score: 0.807\n",
      "Number of features used: 10\n",
      "ridge.coef_: [ 0.07269491 -0.92810378 -1.24237127 -9.65603891  0.94468825 -0.08099605\n",
      "  1.15706149 -0.63971526  3.7388452  -3.52255519]\n",
      "R squared: 0.782\n"
     ]
    }
   ],
   "source": [
    "ridge_scaled = Ridge(alpha=1.0, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(ridge_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(ridge_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(ridge_scaled.coef_ != 0)))\n",
    "print(\"ridge.coef_: {}\".format(ridge_scaled.coef_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(ridge_scaled, X_train_scaled, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:514: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6987.243519083921, tolerance: 6.696182299034478\n",
      "  positive)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:514: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6071.119628578511, tolerance: 6.835283604888848\n",
      "  positive)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:514: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.780\n",
      "Best parameters: 1.0\n",
      "Test set score: 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6123.092976435333, tolerance: 6.7078127114330615\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=Lasso(max_iter=100000), param_grid=dict(alpha=alphas))\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: {}\".format(grid.best_estimator_.alpha))\n",
    "print(\"Test set score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.805\n",
      "Test set score: 0.807\n",
      "Number of features used: 8\n",
      "lasso.coef_: [-3.49967757e-04  2.49295965e-03 -8.87575499e-02 -3.79258543e-01\n",
      "  1.92082688e-03  0.00000000e+00  2.48221832e-03 -0.00000000e+00\n",
      "  4.81687904e-01 -1.86583703e-01]\n",
      "R squared: 0.783\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=1.0, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\n",
    "print(\"lasso.coef_: {}\".format(lasso.coef_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(lasso, X_train, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:514: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6987.243519081855, tolerance: 6.696182299034478\n",
      "  positive)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:514: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6071.119628578561, tolerance: 6.835283604888848\n",
      "  positive)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:514: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.780\n",
      "Best parameters: 1.0\n",
      "Test set score: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6123.092976435199, tolerance: 6.7078127114330615\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=Lasso(max_iter=100000), param_grid=dict(alpha=alphas))\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(\"Best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: {}\".format(grid.best_estimator_.alpha))\n",
    "print(\"Test set score: {:.3f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.797\n",
      "Test set score: 0.786\n",
      "Number of features used: 5\n",
      "lasso.coef_: [ -0.          -0.          -0.         -10.63996226  -0.\n",
      "   0.           0.45852359  -0.22704534   3.32363204  -2.55679942]\n",
      "R squared: 0.780\n"
     ]
    }
   ],
   "source": [
    "lasso_scaled = Lasso(alpha=1.0, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(lasso_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lasso_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso_scaled.coef_ != 0)))\n",
    "print(\"lasso.coef_: {}\".format(lasso_scaled.coef_))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(lasso_scaled, X_train_scaled, y_train, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Introduction\n",
    "\n",
    "This is a demonstration of supervised machine learning methods for a binary/categorical target variable. The data used for this section is imported from the following link, which features various variables measured for red and white wines. The target variable is \"winetype\", which is a new column that was created that labels the observation with either a 0 for white wine and 1 for red wine.\n",
    "\n",
    "Data Source Link: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/Links to an external site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>winetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.069</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.065</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.073</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "5            7.4              0.66         0.00             1.8      0.075   \n",
       "6            7.9              0.60         0.06             1.6      0.069   \n",
       "7            7.3              0.65         0.00             1.2      0.065   \n",
       "8            7.8              0.58         0.02             2.0      0.073   \n",
       "9            7.5              0.50         0.36             6.1      0.071   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "5                 13.0                  40.0   0.9978  3.51       0.56   \n",
       "6                 15.0                  59.0   0.9964  3.30       0.46   \n",
       "7                 15.0                  21.0   0.9946  3.39       0.47   \n",
       "8                  9.0                  18.0   0.9968  3.36       0.57   \n",
       "9                 17.0                 102.0   0.9978  3.35       0.80   \n",
       "\n",
       "   alcohol  quality  winetype  \n",
       "0      9.4        5         1  \n",
       "1      9.8        5         1  \n",
       "2      9.8        5         1  \n",
       "3      9.8        6         1  \n",
       "4      9.4        5         1  \n",
       "5      9.4        5         1  \n",
       "6      9.4        5         1  \n",
       "7     10.0        7         1  \n",
       "8      9.5        7         1  \n",
       "9     10.5        5         1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_red = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')\n",
    "df_red['winetype'] = 1 # adding a new column and assigning 1 for all values\n",
    "\n",
    "df_white = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white\", sep=';')\n",
    "df_white['winetype'] = 0 #adding a new column and assigning 0 for all values\n",
    "\n",
    "df_combined = pd.concat([df_red, df_white], ignore_index=True) #combining both dataframes\n",
    "df_combined.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.069</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.065</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.073</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "5            7.4              0.66         0.00             1.8      0.075   \n",
       "6            7.9              0.60         0.06             1.6      0.069   \n",
       "7            7.3              0.65         0.00             1.2      0.065   \n",
       "8            7.8              0.58         0.02             2.0      0.073   \n",
       "9            7.5              0.50         0.36             6.1      0.071   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "5                 13.0                  40.0   0.9978  3.51       0.56   \n",
       "6                 15.0                  59.0   0.9964  3.30       0.46   \n",
       "7                 15.0                  21.0   0.9946  3.39       0.47   \n",
       "8                  9.0                  18.0   0.9968  3.36       0.57   \n",
       "9                 17.0                 102.0   0.9978  3.35       0.80   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  \n",
       "5      9.4        5  \n",
       "6      9.4        5  \n",
       "7     10.0        7  \n",
       "8      9.5        7  \n",
       "9     10.5        5  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting the data into train and test sets\n",
    "y2 = df_combined['winetype']\n",
    "X2 = df_combined.loc[:, df_combined.columns != 'winetype']\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, random_state=42) \n",
    "\n",
    "X2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[  1.43096916  13.17085779  -0.32841584  -0.12852306  36.51920195\n",
      "    0.06606175  -0.07055161 -19.1782016    8.62450495   8.95890106\n",
      "   -0.47389951   0.07426586]]\n",
      "Training set score: 0.990\n",
      "Test set score: 0.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1e90).fit(X_train2, y_train2)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test2, y_test2)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(logreg, X_train2, y_train2, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 0.42350109  0.08148562 -0.008986   -0.09347198  0.01388506  0.02725462\n",
      "  -0.06039775  0.02776264  0.12456495  0.06096316  0.01292353  0.01470244]]\n",
      "Training set score: 0.942\n",
      "Test set score: 0.930\n",
      "R squared: 0.674\n",
      "\n",
      "logreg .coef_: [[ 0.42359309  0.08151318 -0.00909452 -0.09352192  0.01388649  0.02724537\n",
      "  -0.06037324  0.02772096  0.12430436  0.06088905  0.01295755  0.01472626]]\n",
      "Training set score: 0.942\n",
      "Test set score: 0.930\n",
      "R squared: 0.674\n",
      "\n",
      "logreg .coef_: [[ 0.42005078  0.08018139 -0.00875093 -0.0925929   0.01362327  0.02722884\n",
      "  -0.0604296   0.0275276   0.12305487  0.05972416  0.01569108  0.01510141]]\n",
      "Training set score: 0.942\n",
      "Test set score: 0.930\n",
      "R squared: 0.675\n"
     ]
    }
   ],
   "source": [
    "logreg_l1 = LogisticRegression(C=100, penalty='l1', tol=0.01, solver='saga')\n",
    "logreg_l2 = LogisticRegression(C=100, penalty='l2', tol=0.01, solver='saga')\n",
    "logreg_el = LogisticRegression(C=100, penalty='elasticnet', solver='saga',\n",
    "                                   l1_ratio=0.5, tol=0.01)\n",
    "\n",
    "logreg_l1.fit(X_train2, y_train2)\n",
    "logreg_l2.fit(X_train2, y_train2)\n",
    "logreg_el.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg_l1.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(logreg_l1.score(X_train2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_l1.score(X_test2, y_test2)))\n",
    "print(\"R squared: {:.3f}\\n\".format(np.mean(cross_val_score(logreg_l1, X_train2, y_train2, cv=10, scoring=\"r2\"))))\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg_l2.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(logreg_l2.score(X_train2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_l2.score(X_test2, y_test2)))\n",
    "print(\"R squared: {:.3f}\\n\".format(np.mean(cross_val_score(logreg_l2, X_train2, y_train2, cv=10, scoring=\"r2\"))))\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg_el.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(logreg_el.score(X_train2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_el.score(X_test2, y_test2)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(logreg_el, X_train2, y_train2, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.969\n",
      "Test set score: 0.936\n",
      "R squared: 0.656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn2 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn2.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(knn2.score(X_train2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(knn2.score(X_test2, y_test2)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(knn2, X_train2, y_train2, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data using StandardScaler\n",
    "\n",
    "When comparing the set scores (test and training) and the overall r^2 between the non-scaled and scaled results, all show improvements to varying extent. This makes sense because standard scaler conceptually transforms that data, where it has a mean of 0 and standard deviation of 1. In other words, it rearranges the data to a normal distribution. As such, it proves to be more effective for reuglarized models in this case (more signifciant improvement for the r squared value for penalized models and KNN classification), considering the target variable is binominal in natural (binary: 0 or 1).\n",
    "\n",
    "1) Logistic Regression\n",
    "<br>\n",
    "Training set score: 0.990 to 0.994\n",
    "<br>\n",
    "Test set score: 0.987 to 0.991\n",
    "<br>\n",
    "R squared: 0.938 to 0.965\n",
    "\n",
    "2) Penalized Logistic Regression L1\n",
    "<br>\n",
    "Training set score: 0.942 to 0.993\n",
    "<br>\n",
    "Test set score: 0.930 to 0.988\n",
    "<br>\n",
    "R squared: 0.675 to 0.959\n",
    "\n",
    "3) Penalized Logistic Regression L2\n",
    "<br>\n",
    "Training set score: 0.942 to 0.993\n",
    "<br>\n",
    "Test set score: 0.930 to 0.989\n",
    "<br>\n",
    "R squared: 0.674 to 0.959\n",
    "\n",
    "4) Penalized Logistic Regression Elasticnet\n",
    "<br>\n",
    "Training set score: 0.942 to 0.993\n",
    "<br>\n",
    "Test set score: 0.930 to 0.998\n",
    "<br>\n",
    "R squared: 0.675 to 0.959\n",
    "\n",
    "5) KNN Classification\n",
    "<br>\n",
    "Trainining set score: 0.969 to 0.997\n",
    "<br>\n",
    "Test set score: 0.936 to 0.990\n",
    "<br>\n",
    "R squared: 0.656 to 0.964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train2)\n",
    "X_train_scaled2 = scaler.transform(X_train2)\n",
    "X_test_scaled2 = scaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 0.23559078  1.58962069 -0.28637813 -4.93750147  0.79218326  1.29778593\n",
      "  -3.18115237  4.53143041  0.02047138  0.62833527  1.51163703  0.34671209]]\n",
      "Training set score: 0.994\n",
      "Test set score: 0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jaeha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg_scaled = LogisticRegression(C=1e90).fit(X_train_scaled2, y_train2)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg_scaled.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(logreg_scaled.score(X_train_scaled2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_scaled.score(X_test_scaled2, y_test2)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(logreg_scaled, X_train_scaled2, y_train2, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 1.07199337  1.56941883 -0.29799633 -2.28650117  0.9274505   0.80283647\n",
      "  -2.94765746  2.11590939  0.71593491  0.86009786  0.41863605  0.24125075]]\n",
      "Training set score: 0.993\n",
      "Test set score: 0.989\n",
      "R squared: 0.959\n",
      "\n",
      "logreg .coef_: [[ 1.07307554  1.56911038 -0.30016438 -2.2972451   0.92716902  0.79109616\n",
      "  -2.9362025   2.11685509  0.7166074   0.85891695  0.41915654  0.24339478]]\n",
      "Training set score: 0.993\n",
      "Test set score: 0.988\n",
      "R squared: 0.959\n",
      "\n",
      "logreg .coef_: [[ 1.06848513  1.5673305  -0.30069483 -2.30953016  0.92618916  0.7938456\n",
      "  -2.9362368   2.1304068   0.71314783  0.857098    0.42663898  0.24396544]]\n",
      "Training set score: 0.993\n",
      "Test set score: 0.988\n",
      "R squared: 0.959\n"
     ]
    }
   ],
   "source": [
    "logreg_l1_scaled = LogisticRegression(C=100, penalty='l1', tol=0.01, solver='saga')\n",
    "logreg_l2_scaled = LogisticRegression(C=100, penalty='l2', tol=0.01, solver='saga')\n",
    "logreg_el_scaled = LogisticRegression(C=100, penalty='elasticnet', solver='saga', l1_ratio=0.5, tol=0.01)\n",
    "\n",
    "logreg_l1_scaled.fit(X_train_scaled2, y_train2)\n",
    "logreg_l2_scaled.fit(X_train_scaled2, y_train2)\n",
    "logreg_el_scaled.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg_l1_scaled.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(logreg_l1_scaled.score(X_train_scaled2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_l1_scaled.score(X_test_scaled2, y_test2)))\n",
    "print(\"R squared: {:.3f}\\n\".format(np.mean(cross_val_score(logreg_l1_scaled, X_train_scaled2, y_train2, cv=10, scoring=\"r2\"))))\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg_l2_scaled.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(logreg_l2_scaled.score(X_train_scaled2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_l2_scaled.score(X_test_scaled2, y_test2)))\n",
    "print(\"R squared: {:.3f}\\n\".format(np.mean(cross_val_score(logreg_l2_scaled, X_train_scaled2, y_train2, cv=10, scoring=\"r2\"))))\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg_el_scaled.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(logreg_el_scaled.score(X_train_scaled2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_el_scaled.score(X_test_scaled2, y_test2)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(logreg_el_scaled, X_train_scaled2, y_train2, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.997\n",
      "Test set score: 0.990\n",
      "R squared: 0.964\n"
     ]
    }
   ],
   "source": [
    "knn2_scaled = KNeighborsClassifier(n_neighbors=3)\n",
    "knn2_scaled.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(knn2_scaled.score(X_train_scaled2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(knn2_scaled.score(X_test_scaled2, y_test2)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(knn2_scaled, X_train_scaled2, y_train2, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertuning parameteres using GridSearchCV\n",
    "\n",
    "Even though it was more work, I have applied the GridSearchCV to both scaled and unscaled variables for KNN classification and logistic regression. The results was then compared against the unaltered and unscaled original models. The code and the full process is shown below.\n",
    "\n",
    "In terms of the result, even though the scaled model of KNN classification improved its r squared score by a significant amount, it is not a feasible model as the knn neighbor has only been optimized to be 1 as the best fit. The best logistic regression model proved to be the penalized model with a penalized term of L1 (Lasso) and a C value of 1. There was only a slight improvement in the fit of the model for unscaled variables when the GridSearchCv was applied.\n",
    "\n",
    "1) KNN Classification (Unscaled, Optimized)\n",
    "<br>\n",
    "Trainining set score: 0.969 to 1.000\n",
    "<br>\n",
    "Test set score: 0.936 to 0.945\n",
    "<br>\n",
    "R squared: 0.656 to 0.695\n",
    "\n",
    "KNN Classification (Scaled, Optimized)\n",
    "<br>\n",
    "Trainining set score: 0.969 to 0.999\n",
    "<br>\n",
    "Test set score: 0.936 to 0.991\n",
    "<br>\n",
    "R squared: 0.656 to 0.967\n",
    "\n",
    "2) Penalized Logistic Regression (Unscaled, Optimized)\n",
    "<br>\n",
    "Training set score: 0.942 to 0.943\n",
    "<br>\n",
    "Test set score: 0.930 to 0.930\n",
    "<br>\n",
    "R squared: 0.675 to 0.683\n",
    "\n",
    "Penalized Logistic Regression (Scaled, Optimized)\n",
    "<br>\n",
    "Training set score: 0.942 to 0.993\n",
    "<br>\n",
    "Test set score: 0.930 to 0.989\n",
    "<br>\n",
    "R squared: 0.675 to 0.959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.945\n",
      "best parameters: {'n_neighbors': 1}\n",
      "test-set score: 0.945\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(1, 15, 2)} #np.arange creates sequence of numbers for each k value\n",
    "\n",
    "grid2 = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10)\n",
    "\n",
    "#use meta model methods to fit score and predict model:\n",
    "grid2.fit(X_train2, y_train2)\n",
    "\n",
    "#extract best score and parameter by calling objects \"best_score_\" and \"best_params_\"\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid2.best_score_))\n",
    "print(\"best parameters: {}\".format(grid2.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid2.score(X_test2, y_test2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score:1.000\n",
      "Test set score: 0.945\n",
      "R squared: 0.695\n"
     ]
    }
   ],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn2.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"Training set score:{:.3f}\".format(knn2.score(X_train2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(knn2.score(X_test2, y_test2)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(knn2, X_train2, y_train2, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.994\n",
      "best parameters: {'n_neighbors': 1}\n",
      "test-set score: 0.991\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(1, 15, 2)} #np.arange creates sequence of numbers for each k value\n",
    "\n",
    "grid2 = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10)\n",
    "\n",
    "#use meta model methods to fit score and predict model:\n",
    "grid2.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "#extract best score and parameter by calling objects \"best_score_\" and \"best_params_\"\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid2.best_score_))\n",
    "print(\"best parameters: {}\".format(grid2.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid2.score(X_test_scaled2, y_test2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.999\n",
      "Test set score: 0.991\n",
      "R squared: 0.967\n"
     ]
    }
   ],
   "source": [
    "knn2_scaled = KNeighborsClassifier(n_neighbors=1)\n",
    "knn2_scaled.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(knn2_scaled.score(X_train_scaled2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(knn2_scaled.score(X_test_scaled2, y_test2)))\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(knn2_scaled, X_train_scaled2, y_train2, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best paramters: {'C': 0.01, 'penalty': 'l1'}\n",
      "R Squared: 0.9423234811165846\n"
     ]
    }
   ],
   "source": [
    "grid = {\"C\":[0.001, 0.01, 0.1, 1, 10, 100, 1000], \"penalty\":[\"l1\",\"l2\"]}\n",
    "logreg = LogisticRegression(tol=0.01, solver = 'saga')\n",
    "logreg2 = GridSearchCV(logreg,grid,cv=10)\n",
    "logreg2.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"Best paramters:\",logreg2.best_params_)\n",
    "print(\"R Squared:\",logreg2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 0.40564202  0.00506072  0.         -0.05206556  0.          0.02506152\n",
      "  -0.05808551  0.          0.07222948  0.          0.02874361  0.        ]]\n",
      "Training set score: 0.943\n",
      "Test set score: 0.930\n",
      "R squared: 0.683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_l1 = LogisticRegression(C=0.01, penalty='l1', tol=0.01, solver='saga')\n",
    "\n",
    "logreg_l1.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg_l1.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(logreg_l1.score(X_train2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_l1.score(X_test2, y_test2)))\n",
    "print(\"R squared: {:.3f}\\n\".format(np.mean(cross_val_score(logreg_l1, X_train2, y_train2, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best paramters: {'C': 1, 'penalty': 'l1'}\n",
      "R Squared: 0.9926108374384236\n"
     ]
    }
   ],
   "source": [
    "grid = {\"C\":[0.001, 0.01, 0.1, 1, 10, 100, 1000], \"penalty\":[\"l1\",\"l2\"]}\n",
    "logreg = LogisticRegression(tol=0.01, solver = 'saga')\n",
    "logreg2 = GridSearchCV(logreg,grid,cv=10)\n",
    "logreg2.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "print(\"Best paramters:\",logreg2.best_params_)\n",
    "print(\"R Squared:\",logreg2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 1.02841031  1.53760509 -0.26618522 -2.17365116  0.89247742  0.70690414\n",
      "  -2.86530892  2.01951825  0.71118153  0.85540343  0.35822099  0.22243465]]\n",
      "Training set score: 0.993\n",
      "Test set score: 0.988\n",
      "R squared: 0.959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_l1_scaled = LogisticRegression(C=1, penalty='l1', tol=0.01, solver='saga')\n",
    "\n",
    "logreg_l1_scaled.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg_l1_scaled.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(logreg_l1_scaled.score(X_train_scaled2, y_train2)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_l1_scaled.score(X_test_scaled2, y_test2)))\n",
    "print(\"R squared: {:.3f}\\n\".format(np.mean(cross_val_score(logreg_l1_scaled, X_train_scaled2, y_train2, cv=10, scoring=\"r2\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing impact of cross-validation strategy in GridSearchCV from ‘stratified k-fold’ to ‘kfold’ with shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.909\n",
      "\n",
      "R squared: 0.941\n",
      "\n",
      "R squared: 0.941\n",
      "\n",
      "R squared: 0.674\n",
      "R squared: 0.664\n",
      "R squared: 0.683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "# Set up function parameters for diff't cross validation strategies\n",
    "kfold1 = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "print(\"R squared: {:.3f}\\n\".format(np.mean(cross_val_score(KNeighborsClassifier(n_neighbors=5), X2, y2, cv=kfold1))))\n",
    "#This improves the fit from 0.909 to a stratified k fold fit of 0.674\n",
    "\n",
    "kfold2 = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "print(\"R squared: {:.3f}\\n\".format(np.mean(cross_val_score(KNeighborsClassifier(n_neighbors=5), X2, y2, cv=kfold2))))\n",
    "#By implementing the shuffle, the fit improved from 0.909 to 0.941\n",
    "\n",
    "kfold3 = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "print(\"R squared: {:.3f}\\n\".format(np.mean(cross_val_score(KNeighborsClassifier(n_neighbors=5), X2, y2, cv=kfold3))))\n",
    "#By adding a random state of the shuffle, the fit improved in comparsion to without a shuffle, but stayed the same in relation to without a random state\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X2, y2, random_state=0)\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X2, y2, random_state=1) \n",
    "\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn2.fit(X_train2, y_train2)\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(knn2, X_train2, y_train2, cv=10, scoring=\"r2\"))))\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn3.fit(X_train3, y_train3)\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(knn3, X_train3, y_train3, cv=10, scoring=\"r2\"))))\n",
    "#By changing the random state from 42 to 0, it decreased the fit from 0.674 to 0.664\n",
    "\n",
    "knn4 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn4.fit(X_train4, y_train4)\n",
    "print(\"R squared: {:.3f}\".format(np.mean(cross_val_score(knn4, X_train4, y_train4, cv=10, scoring=\"r2\"))))\n",
    "#By changing the random state from 42 to 1, it improved the fit from 0.674 to 0.683"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
